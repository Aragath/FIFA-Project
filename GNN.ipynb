{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install arm-mango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torcheck #### Check for potential issue in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ogb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"pre-processed/fixtures_full.csv\"\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "data['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.where(data.applymap(lambda x: x == ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(pd.isnull(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GNN\n",
    "Using hierachical pooling to reduce the graph until it's only one embedding\n",
    "- Hierachical pooling: reduce the nodes by distribute node to neighbor nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "from torch.nn import Linear, BatchNorm1d, ModuleList\n",
    "from torch_geometric.nn import TransformerConv, TopKPooling \n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.nn import BatchNorm, PNAConv, global_add_pool\n",
    "#torch.manual_seed(42)\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, feature_size, model_params):\n",
    "        super(GNN, self).__init__()\n",
    "        embedding_size = model_params[\"model_embedding_size\"]\n",
    "        n_heads = model_params[\"model_attention_heads\"]\n",
    "        self.n_layers = model_params[\"model_layers\"]\n",
    "        dropout_rate = model_params[\"model_dropout_rate\"]\n",
    "        top_k_ratio = model_params[\"model_top_k_ratio\"]\n",
    "        self.top_k_every_n = model_params[\"model_top_k_every_n\"]\n",
    "        dense_neurons = model_params[\"model_dense_neurons\"]\n",
    "        #edge_dim = model_params[\"model_edge_dim\"]\n",
    "        \n",
    "\n",
    "        self.conv_layers = ModuleList([])\n",
    "        self.transf_layers = ModuleList([])\n",
    "        self.pooling_layers = ModuleList([])\n",
    "        self.bn_layers = ModuleList([])\n",
    "\n",
    "        # Transformation layer: transform original node features to embedding vector(size: embedding_size(defined in config.py))\n",
    "        self.conv1 = TransformerConv(feature_size, \n",
    "                                    embedding_size, \n",
    "                                    heads=n_heads, \n",
    "                                    dropout=dropout_rate,\n",
    "                                    #edge_dim=edge_dim,\n",
    "                                    beta=True) \n",
    "\n",
    "        self.transf1 = Linear(embedding_size*n_heads, embedding_size)\n",
    "        self.bn1 = BatchNorm1d(embedding_size)\n",
    "\n",
    "        # Other layers: message passing and pooling\n",
    "        for i in range(self.n_layers):\n",
    "            self.conv_layers.append(TransformerConv(embedding_size, \n",
    "                                                    embedding_size, \n",
    "                                                    heads=n_heads, \n",
    "                                                    dropout=dropout_rate,\n",
    "                                                    #edge_dim=edge_dim,\n",
    "                                                    beta=True))\n",
    "\n",
    "            # map conv_layer output size back to emgedding_size(embedding_size*n_heads -> embedding_size)\n",
    "            self.transf_layers.append(Linear(embedding_size*n_heads, embedding_size))\n",
    "            # Batch normalization\n",
    "            self.bn_layers.append(BatchNorm1d(embedding_size))\n",
    "            # Top-k pooling to reduce the size of the graph\n",
    "            if i % self.top_k_every_n == 0:\n",
    "                self.pooling_layers.append(TopKPooling(embedding_size, ratio=top_k_ratio))\n",
    "            \n",
    "\n",
    "        # Linear output layers: feed graph representation in & reduce until single value left\n",
    "        self.linear1 = Linear(embedding_size*2, dense_neurons)\n",
    "        self.linear2 = Linear(dense_neurons, int(dense_neurons/2))  \n",
    "        #self.linear3 = Linear(int(dense_neurons/2), 1)  \n",
    "        self.linear3 = Linear(int(dense_neurons/2), 3) # same as the general form\n",
    "\n",
    "    #def forward(self, x, edge_index):\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "    #def forward(self, x, edge_attr=None, edge_index, batch_index):\n",
    "        # Initial transformation\n",
    "        #x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(self.transf1(x))\n",
    "        x = torch.relu((x))\n",
    "\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        # Holds the intermediate graph representations\n",
    "        global_representation = []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            #x = self.conv_layers[i](x, edge_index, edge_attr)\n",
    "            x = self.conv_layers[i](x, edge_index)\n",
    "            x = torch.relu(self.transf_layers[i](x))\n",
    "            x = torch.relu((x))\n",
    "            x = self.bn_layers[i](x)\n",
    "            # Always aggregate last layer\n",
    "            if i % self.top_k_every_n == 0 or i == self.n_layers:\n",
    "                x , edge_index, edge_attr, batch_index, _, _ = self.pooling_layers[int(i/self.top_k_every_n)](  x, \n",
    "                                                                                                                edge_index, \n",
    "                                                                                                                None, \n",
    "                                                                                                                batch_index)              \n",
    "                # Add current representation\n",
    "                global_representation.append(torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1))\n",
    "    \n",
    "        x = sum(global_representation)\n",
    "\n",
    "        # Output block\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = F.dropout(x, p=0.8, training=self.training)\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x = F.dropout(x, p=0.8, training=self.training)\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train Epoch, Test, Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import torch \n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, f1_score, \\\n",
    "    accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from processedDataset import ProcessedDataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from model import GNN\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify tracking server\n",
    "#mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_one_epoch(epoch, model, train_loader, optimizer, loss_fn):\n",
    "    # Enumerate over the data\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "    for _, batch in enumerate(tqdm(train_loader)):\n",
    "        batch.x = torch.tensor(batch.x)\n",
    "        batch.x = batch.x.reshape((-1, *batch.x.shape[2:]))\n",
    "        #print(\"this is batch.x shape: \", batch.x.shape)\n",
    "        # Use GPU\n",
    "        batch.to(device)  \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad() \n",
    "        # Passing the node features and the connection info\n",
    "        pred = model(torch.tensor(batch.x).float(), \n",
    "                                #batch.edge_attr.float(),\n",
    "                                batch.edge_index, \n",
    "                                batch.batch) \n",
    "        # Calculating the loss and gradients\n",
    "        #loss = loss_fn(torch.squeeze(pred), batch.y.float()) # has error if loss_fn changed to cross_entropy\n",
    "        loss = torch.sqrt(loss_fn(pred, batch.y.long()))\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update tracking\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "        \n",
    "#         predict = torch.sigmoid(pred).cpu().detach().numpy()\n",
    "\n",
    "        #all_preds.append(np.rint(torch.sigmoid(pred).cpu().detach().numpy()))\n",
    "        all_preds.append(np.argmax(pred.cpu().detach().numpy(), axis=1))\n",
    "        \n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    calculate_metrics(all_preds, all_labels, epoch, \"train\")\n",
    "    return running_loss/step\n",
    "\n",
    "def test(epoch, model, test_loader, loss_fn):\n",
    "    all_preds = []\n",
    "    all_preds_raw = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "    for batch in test_loader:\n",
    "        batch.x = torch.tensor(batch.x)\n",
    "        batch.x = batch.x.reshape((-1, *batch.x.shape[2:]))\n",
    "        batch.to(device)  \n",
    "        pred = model(torch.tensor(batch.x).float(), \n",
    "                        #batch.edge_attr.float(),\n",
    "                        batch.edge_index, \n",
    "                        batch.batch) \n",
    "        #loss = loss_fn(torch.squeeze(pred), batch.y.float()) # has error if loss_fn changed to cross_entropy\n",
    "        loss = torch.sqrt(loss_fn(pred, batch.y.long()))\n",
    "         # Update tracking\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "        \n",
    "#         predict = torch.sigmoid(pred).cpu().detach().numpy()\n",
    "        \n",
    "        #all_preds.append(np.rint(torch.sigmoid(pred).cpu().detach().numpy()))\n",
    "        all_preds.append(np.argmax(pred.cpu().detach().numpy(), axis=1))\n",
    "        all_preds_raw.append(torch.sigmoid(pred).cpu().detach().numpy())\n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    print(all_preds_raw[0][:10])\n",
    "    print(all_preds[:10])\n",
    "    print(all_labels[:10])\n",
    "    calculate_metrics(all_preds, all_labels, epoch, \"test\")\n",
    "    log_conf_matrix(all_preds, all_labels, epoch)\n",
    "    return running_loss/step\n",
    "\n",
    "def final_test(epoch, model, final_test_loader, threshold):\n",
    "    all_preds = []\n",
    "    all_preds_raw = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in final_test_loader:\n",
    "        batch.x = torch.tensor(batch.x)\n",
    "        batch.x = batch.x.reshape((-1, *batch.x.shape[2:]))\n",
    "        batch.to(device)  \n",
    "        pred = model(torch.tensor(batch.x).float(), \n",
    "                        #batch.edge_attr.float(),\n",
    "                        batch.edge_index, \n",
    "                        batch.batch) \n",
    "\n",
    "        all_preds.append(np.argmax(pred.cpu().detach().numpy(), axis=1))\n",
    "        all_preds_raw.append(torch.sigmoid(pred).cpu().detach().numpy())\n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "    \n",
    "    \n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    \n",
    "    # Pick the top N confidence\n",
    "    num_threshold = int(len(all_preds) * threshold)\n",
    "    ind = all_preds # (463, )\n",
    "    raw_nums = np.array(sum([l.tolist() for l in all_preds_raw], [])) #( 463 * 3)\n",
    "    conf_nums = [] # the confidence of that predicted label ex. ind = 2 & raw_nums = [0.4, 0.45, 0.5] -> 0.5\n",
    "    for i, list_ in zip(ind, raw_nums):\n",
    "        conf_nums.append(list_[i])\n",
    "    #print(\"conf_nums: \", conf_nums)\n",
    "    \n",
    "    # get the top N conf_nums' indices\n",
    "    indices = np.argpartition(conf_nums, -num_threshold)[-num_threshold:]\n",
    "    #print(\"indices: \", indices)\n",
    "    top_preds = all_preds[indices]\n",
    "    top_labels = all_labels[indices]\n",
    "    #print(\"top_preds: \", top_preds, \"| top_labels: \", top_labels)\n",
    "    \n",
    "    # calculate acc\n",
    "    acc = accuracy_score(top_labels, top_preds)\n",
    "    mlflow.log_metric(key=\"Final Test Accuracy\", value=float(acc), step=epoch)\n",
    "\n",
    "    print(\"final acc: \", acc)\n",
    "\n",
    "def log_conf_matrix(y_pred, y_true, epoch):\n",
    "    # Log confusion matrix as image\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    classes = [\"0\", \"1\", \"2\"]\n",
    "    df_cfm = pd.DataFrame(cm, index = classes, columns = classes)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    cfm_plot = sns.heatmap(df_cfm, annot=True, cmap='Blues', fmt='g')\n",
    "    cfm_plot.set_ylabel('True label')\n",
    "    cfm_plot.set_xlabel('Predicted label')\n",
    "    cfm_plot.figure.savefig(f'data/images/cm_{epoch}.png')\n",
    "    mlflow.log_artifact(f\"data/images/cm_{epoch}.png\")\n",
    "\n",
    "def calculate_metrics(y_pred, y_true, epoch, type):\n",
    "    try:\n",
    "        print(f\"\\n Confusion matrix: \\n {confusion_matrix(y_pred, y_true)}\")##############################\n",
    "    except:\n",
    "        print(\"Error with confusion matrix\")\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    print(f\"Precision: {prec}\")\n",
    "    print(f\"Recall: {rec}\")\n",
    "    mlflow.log_metric(key=f\"F1-{type}\", value=float(f1), step=epoch)\n",
    "    mlflow.log_metric(key=f\"Accuracy-{type}\", value=float(acc), step=epoch)\n",
    "    mlflow.log_metric(key=f\"Precision-{type}\", value=float(prec), step=epoch)\n",
    "    mlflow.log_metric(key=f\"Recall-{type}\", value=float(rec), step=epoch)\n",
    "    # try:\n",
    "    #     roc = roc_auc_score(y_true, y_pred)\n",
    "    #     print(f\"ROC AUC: {roc}\")\n",
    "    #     mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=float(roc), step=epoch)\n",
    "    # except:\n",
    "    #     #mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=float(0), step=epoch)\n",
    "    #     print(f\"ROC AUC: notdefined\")\n",
    "    \n",
    "def predict(model, test_loader):\n",
    "    all_preds = []\n",
    "    all_preds_raw = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in test_loader:\n",
    "        batch.x = torch.tensor(batch.x)\n",
    "        batch.x = batch.x.reshape((-1, *batch.x.shape[2:]))\n",
    "        batch.to(device)  \n",
    "        pred = model(torch.tensor(batch.x).float(), \n",
    "                        #batch.edge_attr.float(),\n",
    "                        batch.edge_index, \n",
    "                        batch.batch) \n",
    "\n",
    "        all_preds.append(np.argmax(pred.cpu().detach().numpy(), axis=1))\n",
    "        all_preds_raw.append(torch.sigmoid(pred).cpu().detach().numpy())\n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    return all_preds, all_preds_raw, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "from mango import scheduler, Tuner\n",
    "from config import HYPERPARAMETERS, BEST_PARAMETERS, SIGNATURE\n",
    "from sklearn.utils import class_weight\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def run_one_training(params):\n",
    "    params = params[0]\n",
    "    with mlflow.start_run() as run:\n",
    "        # Log parameters used in this experiment\n",
    "        for key in params.keys():\n",
    "            mlflow.log_param(key, params[key])\n",
    "\n",
    "        # Loading the dataset\n",
    "        # Graph-level tasks: Graph classification - https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html#Graph-level-tasks:-Graph-classification \n",
    "        print(\"Loading dataset...\")\n",
    "        full_dataset = ProcessedDataset(root = \"data/\", filename = \"fixtures_full.csv\")\n",
    "        full_dataset.shuffle()\n",
    "        train_dataset = full_dataset[:3400] # around 80% of the full dataset\n",
    "        test_dataset = full_dataset[3400:3800]\n",
    "        final_test_dataset = full_dataset[3800:]\n",
    "        \n",
    "        #train_size = int(0.8 * len(full_dataset))\n",
    "        #test_size = len(full_dataset) - train_size\n",
    "        #print(\"size of train and test: \", train_size, test_size)\n",
    "        #train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "        #print(test_dataset)\n",
    "        \n",
    "        # Prepare training\n",
    "        print(\"Preparing Training\")\n",
    "        batch_size=params[\"batch_size\"]\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)  \n",
    "        final_test_loader = DataLoader(final_test_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # # for checking dataloader\n",
    "        # dataiter = iter(train_loader)\n",
    "        # data = next(dataiter)\n",
    "        # features, labels, aa, *_ = data\n",
    "        # print(\"features: \", features)\n",
    "        # print(\"labels: \", labels)\n",
    "        # print(\"aa: \", aa)\n",
    "        # print(np.array(features[1]).shape)\n",
    "        # print(labels[0])\n",
    "        # print(labels[1].shape)\n",
    "        # print(aa[1])\n",
    "        \n",
    "        # Loading the model\n",
    "        print(\"Loading model...\")\n",
    "        model_params = {k: v for k, v in params.items() if k.startswith(\"model_\")}\n",
    "        model = GNN(feature_size=train_dataset[0].x.shape[1], model_params=model_params)\n",
    "\n",
    "        print(model)\n",
    "        model = model.to(device)\n",
    "        print(f\"Number of parameters: {count_parameters(model)}\")\n",
    "        mlflow.log_param(\"num_params\", count_parameters(model))\n",
    "\n",
    "        # < 1 increases precision, > 1 recall\n",
    "        #weight = torch.tensor([params[\"pos_weight\"]], dtype=torch.float32).to(device)\n",
    "        #loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=weight)\n",
    "        \n",
    "#         # add in weight of class according to train dataset\n",
    "#         train_indices = train_dataset.indices\n",
    "        \n",
    "#         DATA_PATH = \"pre-processed/fixtures_full.csv\"\n",
    "#         total = pd.read_csv(DATA_PATH)\n",
    "#         train_ds_bincount = np.bincount(total.iloc[train_indices]['result'])\n",
    "#         #print(train_ds_bincount)\n",
    "\n",
    "#         target = total.iloc[train_indices]['result']\n",
    "#         #print(target)\n",
    "#         class_weights= class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(target), y=target)\n",
    "#         class_weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "        #print(class_weights) # etc. ([1.0239, 1.2753, 0.8070])\n",
    "        class_weights = [1.0239, 1.2753, 0.8070]\n",
    "        class_weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "        \n",
    "        #loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), \n",
    "                                    lr=params[\"learning_rate\"], \n",
    "                                    weight_decay=params[\"weight_decay\"], \n",
    "                                    momentum=params[\"sgd_momentum\"])\n",
    "        # optimizer = torch.optim.AdamW(model.parameters(), \n",
    "        #                               lr=params[\"learning_rate\"], \n",
    "        #                               weight_decay=params[\"weight_decay\"])\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=params[\"scheduler_gamma\"])\n",
    "        \n",
    "        # Start training\n",
    "        best_loss = 1000\n",
    "        early_stopping_counter = 0\n",
    "        for epoch in range(3000): \n",
    "            if early_stopping_counter <= 20: # = x * 5 \n",
    "                # Training\n",
    "                model.train()\n",
    "                loss = train_one_epoch(epoch, model, train_loader, optimizer, loss_fn)\n",
    "                print(f\"Epoch {epoch} | Train Loss {loss}\")\n",
    "                mlflow.log_metric(key=\"Train loss\", value=float(loss), step=epoch)\n",
    "\n",
    "                # Testing\n",
    "                model.eval()\n",
    "                if epoch % 5 == 0:\n",
    "                    loss = test(epoch, model, test_loader, loss_fn)\n",
    "                    print(f\"Epoch {epoch} | Test Loss {loss}\")\n",
    "                    mlflow.log_metric(key=\"Test loss\", value=float(loss), step=epoch)\n",
    "                    \n",
    "                    # Update best loss\n",
    "                    if float(loss) < best_loss:\n",
    "                        best_loss = loss\n",
    "                        # Save the currently best model \n",
    "                        mlflow.pytorch.log_model(model, \"model\", signature=SIGNATURE)\n",
    "                        early_stopping_counter = 0\n",
    "                    else:\n",
    "                        early_stopping_counter += 1\n",
    "\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                print(\"Early stopping due to no improvement.\")\n",
    "                # model.eval()\n",
    "                # final_test(epoch, model, final_test_loader, threshold=params[\"threshold\"])\n",
    "                return [best_loss]\n",
    "        print(f\"Finishing training with best test loss: {best_loss}\")\n",
    "        # model.eval()\n",
    "        # final_test(3000, model, final_test_loader, threshold=params[\"threshold\"])\n",
    "        return [best_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameter search\n",
    "print(\"Running hyperparameter search...\")\n",
    "config = dict()\n",
    "config[\"optimizer\"] = \"Bayesian\"\n",
    "config[\"num_iteration\"] = 100\n",
    "\n",
    "tuner = Tuner(HYPERPARAMETERS, \n",
    "              objective=run_one_training,\n",
    "              conf_dict=config) \n",
    "results = tuner.minimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logged_model = 'runs:/de621c161db0430db8ed68cb52120e8f/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pytorch.load_model(logged_model)\n",
    "for conv in loaded_model.conv_layers:\n",
    "    conv.aggr_module = torch_geometric.nn.aggr.SumAggregation()\n",
    "\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processedDataset import ProcessedDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "full_dataset = ProcessedDataset(root = \"data/\", filename = \"fixtures_full.csv\")\n",
    "full_dataset.shuffle()\n",
    "dataset = full_dataset[3800:]\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=len(dataset))\n",
    "\n",
    "all_pred, all_pred_raw, all_label = predict(loaded_model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### General Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check mlflow\n",
    "!mlflow --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
    "from torch_geometric.nn import TransformerConv, GATConv, TopKPooling, BatchNorm\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, feature_size):\n",
    "        super(GNN, self).__init__()\n",
    "        num_classes = 3\n",
    "        embedding_size = 1024\n",
    "\n",
    "        # GNN layers\n",
    "        self.conv1 = GATConv(feature_size, embedding_size, heads=3, dropout=0.3)\n",
    "        self.head_transform1 = Linear(embedding_size*3, embedding_size)\n",
    "        self.pool1 = TopKPooling(embedding_size, ratio=0.8)\n",
    "        self.conv2 = GATConv(embedding_size, embedding_size, heads=3, dropout=0.3)\n",
    "        self.head_transform2 = Linear(embedding_size*3, embedding_size)\n",
    "        self.pool2 = TopKPooling(embedding_size, ratio=0.5)\n",
    "        self.conv3 = GATConv(embedding_size, embedding_size, heads=3, dropout=0.3)\n",
    "        self.head_transform3 = Linear(embedding_size*3, embedding_size)\n",
    "        self.pool3 = TopKPooling(embedding_size, ratio=0.2)\n",
    "\n",
    "        # Linear layers\n",
    "        self.linear1 = Linear(embedding_size*2, 1024) # 1024 dense neurons\n",
    "        self.linear2 = Linear(1024, num_classes)   \n",
    "\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # def forward(self, x, edge_attr, edge_index, batch_index):\n",
    "        # First block\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.head_transform1(x)\n",
    "\n",
    "        x, edge_index, edge_attr, batch_index, _, _ = self.pool1(x, \n",
    "                                                                 edge_index,\n",
    "                                                                 None,\n",
    "                                                                 batch_index)\n",
    "        x1 = torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1)\n",
    "\n",
    "        # Second block\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.head_transform2(x)\n",
    "        x, edge_index, edge_attr, batch_index, _, _ = self.pool2(x,\n",
    "                                                                edge_index,\n",
    "                                                                None,\n",
    "                                                                batch_index)\n",
    "        x2 = torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1)\n",
    "\n",
    "        # Third block\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.head_transform3(x)\n",
    "        x, edge_index, edge_attr, batch_index, _, _ = self.pool3(x,\n",
    "                                                                edge_index,\n",
    "                                                                None,\n",
    "                                                                batch_index)\n",
    "        x3 = torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1)\n",
    "\n",
    "        # Concat pooled vectors\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        # Output block\n",
    "        x = self.linear1(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import torch \n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, f1_score, \\\n",
    "    accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from processedDataset import ProcessedDataset\n",
    "from model import GNN\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = ProcessedDataset(root = \"data/\", filename = \"fixtures_train.csv\")\n",
    "test_dataset = ProcessedDataset(root = \"data/\", filename = \"fixtures_test.csv\", test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(feature_size=train_dataset[0].x.shape[1])\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Number of parameters: {count_parameters(model)}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer, could try some more to see which is better\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) # or ADAM...etc\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training\n",
    "NUM_GRAPHS_PER_BATCH = 256\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                        batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "def train(epoch):\n",
    "    # Enumerate over the data\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for _, batch in enumerate(tqdm(train_loader)):\n",
    "        # use CPU(or GPU)\n",
    "        batch.to(device)\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # passing the node features and the connection info\n",
    "        pred = model(batch.x.float(), \n",
    "                   # batch.edge_attr.float(), \n",
    "                    batch.edge_index, \n",
    "                    batch.batch)\n",
    "        # calculating the loss and gradients\n",
    "        loss = torch.sqrt(loss_fn(pred, batch.y))\n",
    "        loss.backward()\n",
    "        # update using the gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        all_preds.append(np.argmax(pred.cpu().detach().numpy(), axis=1))\n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    calculate_metrics(all_preds, all_labels, epoch, \"train\")\n",
    "    return loss\n",
    "\n",
    "def test(epoch):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch in test_loader:\n",
    "        batch.to(device)\n",
    "        pred = model(batch.x.float(), \n",
    "                   # batch.edge_attr.float(), \n",
    "                    batch.edge_index, \n",
    "                    batch.batch)\n",
    "        loss = torch.sqrt(loss_fn(pred, batch.y))\n",
    "        all_preds.append(np.argmax(pred.cpu().detach().numpy(), axis=1))\n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "        \n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    calculate_metrics(all_preds, all_labels, epoch, \"test\")\n",
    "    return loss\n",
    "\n",
    "def calculate_metrics(y_pred, y_true, epoch, type):\n",
    "    print(f\"\\n Confusion matrix: \\n {confusion_matrix(y_pred, y_true)}\")\n",
    "    print(f\"F1 Score: {f1_score(y_pred, y_true, average='macro')}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_pred, y_true)}\")\n",
    "    print(f\"Precision: {precision_score(y_pred, y_true, average='macro')}\")\n",
    "    print(f\"Recall: {recall_score(y_pred, y_true, average='macro')}\")\n",
    "    try:\n",
    "        roc = roc_auc_score(y_pred, y_true)\n",
    "        print(f\"ROC AUC: {roc}\")\n",
    "        mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=float(roc), step=epoch)\n",
    "    except:\n",
    "        mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=float(0), step=epoch)\n",
    "        print(f\"ROC AUC: notdefined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run the training\n",
    "# Run the training\n",
    "with mlflow.start_run() as run:\n",
    "    for epoch in range(500): \n",
    "        # Training\n",
    "        model.train()\n",
    "        loss = train(epoch=epoch)\n",
    "        loss = loss.detach().cpu().numpy()\n",
    "        print(f\"Epoch {epoch} | Train Loss {loss}\")\n",
    "        mlflow.log_metric(key=\"Train loss\", value=float(loss), step=epoch)\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        if epoch % 5 == 0:\n",
    "            loss = test(epoch=epoch)\n",
    "            loss = loss.detach().cpu().numpy()\n",
    "            print(f\"Epoch {epoch} | Test Loss {loss}\")\n",
    "            mlflow.log_metric(key=\"Test loss\", value=float(loss), step=epoch)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "mlflow.pytorch.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "b075c2ba4667a0fecac182d8c53ecf752516f400485e55a5d8e8c5c7f6d3bc3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
